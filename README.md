# ü©∫ Diabetes Risk Screening using Machine Learning

An end-to-end machine learning application that estimates **diabetes risk** from basic
patient health indicators.  
The project emphasizes **practical ML system design**, focusing on evaluation,
interpretability, API-based deployment, and cloud-hosted inference rather than
model training alone.

‚ö†Ô∏è This application is built for learning and demonstration purposes only and is **not medical advice**.

---

## Problem Statement
Diabetes is a chronic condition where early screening and risk awareness can support
timely intervention. Using historical patient health data, this project builds a
binary classification system to estimate whether an individual is at **higher or lower
risk** of diabetes based on commonly available medical features.

**Target definition**
- `1` ‚Üí Higher diabetes risk  
- `0` ‚Üí Lower diabetes risk  

---

## Dataset
The project uses the **PIMA Indians Diabetes Dataset**, which contains patient-level
medical attributes commonly used for diabetes risk modeling.

**Features**
- Pregnancies  
- Glucose  
- Blood Pressure  
- Skin Thickness  
- Insulin  
- BMI  
- Diabetes Pedigree Function  
- Age  

Several medical features contain **invalid zero values** (e.g., glucose or BMI = 0).
These values are treated as missing during preprocessing to better reflect real-world
data quality issues.

---

## Modeling Approach
The project follows a complete machine learning lifecycle:

- Invalid zero values are replaced with missing values and imputed using median statistics  
- Numerical features are standardized to support distance-based models  
- A **Support Vector Machine (SVM)** classifier is trained using a pipeline that combines
  preprocessing and modeling  
- Hyperparameters such as kernel type, regularization strength, and class weighting are
  tuned using cross-validation  

To improve interpretability and downstream decision-making, predicted probabilities
are **calibrated** before deployment.  
Rather than using a fixed 0.50 cutoff, the decision threshold is selected using
**recall-oriented evaluation**, aligning the model with screening-style objectives.

---

## Model Evaluation
The model is evaluated using multiple complementary metrics:

- ROC-AUC  
- Precision‚ÄìRecall AUC  
- Precision, Recall, and Accuracy  
- Confusion Matrix  

Evaluation artifacts and the selected screening threshold are stored and reused during
inference, improving transparency and reproducibility.  
This highlights the trade-offs involved in healthcare-style classification problems,
where minimizing false negatives is often more important than raw accuracy.

---

## API-Based Inference (FastAPI)
The trained and calibrated model is deployed behind a **FastAPI-based inference service**,
demonstrating a production-style ML backend.

The API:
- Loads the trained ML pipeline once at startup  
- Validates inputs using Pydantic schemas  
- Exposes prediction endpoints for real-time inference  
- Provides automatic OpenAPI (Swagger) documentation  
- Separates model inference from the user interface  

This design mirrors real-world ML systems where models are accessed through APIs rather
than embedded directly in frontend applications.

---

## Streamlit Frontend (User Interface)
A **Streamlit-based frontend** serves as the user-facing interface for the system.

The application allows users to:
- Enter patient health information  
- Trigger real-time predictions via the FastAPI backend  
- View estimated diabetes risk probabilities  
- See screening results categorized as **Lower**, **Moderate**, or **Higher** risk  
- Inspect raw API responses for transparency  

All predictions are generated by calling the backend API, ensuring a clean separation
between the frontend and model inference logic.

---
## Cloud Deployment
The project is deployed using a **decoupled frontend‚Äìbackend architecture**:

- **FastAPI backend:** Deployed on **Render** as a cloud-hosted inference service  
- **Streamlit frontend:** Deployed on **Streamlit Cloud**, configured to consume the API  
- **Configuration:** API URLs managed using environment variables and Streamlit secrets  

This setup reflects real-world ML systems where model inference is served via
backend APIs and accessed by lightweight frontend applications.

## Key Learnings
- Real-world datasets require careful preprocessing and validation  
- Probability calibration improves interpretability of model outputs  
- Threshold selection is critical for screening-style applications  
- Separating frontend and backend improves scalability and maintainability  
- Deploying ML systems surfaces practical issues beyond model performance  

---

## Future Improvements
- Add baseline model comparisons (Logistic Regression, Tree-based models)  
- Incorporate explainability techniques (feature importance, SHAP)  
- Add monitoring and logging for production inference  
- Extend deployment with containerization and CI/CD  

---

## Disclaimer
This project is for educational and demonstration purposes only.
It is not intended for medical diagnosis or clinical decision-making.
